# Project Summary


## Project Statistics
| Metric              | Value         |
|---------------------|---------------|
| Total Files         | 12         |
| Total Lines of Code | 1424       |
| Total Size          | 46.89 KB   |

## Language Distribution
| Language        | Files |
|-----------------|-------|
| C++             | 4     |
| C++ Header      | 3     |
| Text            | 1     |
| JSON            | 1     |
| Go              | 1     |
| Shell Script    | 1     |
| Python          | 1     |

## Directory Structure
```
secret-hound/
â”œâ”€â”€ bin
â”‚   â””â”€â”€ secret-hound
â”œâ”€â”€ rules
â”‚   â””â”€â”€ default.json
â”œâ”€â”€ src
â”‚   â”œâ”€â”€ git_analyzer
â”‚   â”‚   â””â”€â”€ main.go
â”‚   â”œâ”€â”€ hound_core
â”‚   â”‚   â”œâ”€â”€ rule_parser.cpp
â”‚   â”‚   â”œâ”€â”€ rule_parser.hpp
â”‚   â”‚   â”œâ”€â”€ scanner.cpp
â”‚   â”‚   â”œâ”€â”€ scanner.hpp
â”‚   â”‚   â”œâ”€â”€ threadpool.cpp
â”‚   â”‚   â””â”€â”€ threadpool.hpp
â”‚   â””â”€â”€ main.cpp
â”œâ”€â”€ test
â”‚   â””â”€â”€ secret_hound_test.sh
â””â”€â”€ ui
    â””â”€â”€ reporter.py
```
---
--- START OF FILE bin/secret-hound ---
--- File: bin/secret-hound | Language: Text ---

#!/usr/bin/env python3
# File: tools/secret-hound/bin/secret-hound
# Description: Main user-facing entry point (UPDATED to fix rich TypeError).

import sys
import argparse
import subprocess
import json
import os
from pathlib import Path
from collections import defaultdict

# --- START: Core SNIPER Environment Integration ---
try:
    _PROJECT_ROOT = Path(__file__).resolve().parents[3]
    sys.path.insert(0, str(_PROJECT_ROOT))
    
    from lib.sniper_env import env
    from lib.help_renderer import render_help

    from rich.console import Console, Group
    from rich.table import Table
    from rich.panel import Panel
    from rich.text import Text
    from rich.syntax import Syntax
    from rich.rule import Rule
    from rich.status import Status

    env.log.name = "secret-hound"
except (ImportError, IndexError):
    print("\033[91m[CRITICAL ERROR] Could not initialize SNIPER environment or dependencies (rich).\033[0m", file=sys.stderr)
    sys.exit(1)
# --- END: Core SNIPER Environment Integration ---


def get_tool_path(tool_name: str) -> str:
    """Finds the absolute path to a binary in the main sniper bin directory."""
    path = env.ROOT_DIR / "bin" / tool_name
    if not path.is_file() or not os.access(path, os.X_OK):
        path = env.ROOT_DIR / "tools" / "secret-hound" / "bin" / tool_name
        if not path.is_file() or not os.access(path, os.X_OK):
             raise FileNotFoundError(f"Required binary '{tool_name}' not found or is not executable.")
    return str(path)

def run_backend_process(command: list, console: Console):
    """
    Runs a backend process, captures its stdout line-by-line, and yields JSON objects.
    """
    try:
        process = subprocess.Popen(
            command,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            encoding='utf-8'
        )

        for line in process.stdout:
            try:
                yield json.loads(line)
            except json.JSONDecodeError:
                continue

        process.wait()
        if process.returncode != 0:
            stderr_output = process.stderr.read()
            console.print(f"[bold red]Backend scanner exited with an error (code {process.returncode}):[/]", style="stderr")
            console.print(Panel(stderr_output, border_style="red"), style="stderr")

    except FileNotFoundError:
         env.log.error(f"Backend command not found: '{command[0]}'")
    except Exception as e:
        env.log.error(f"An unexpected error occurred while running backend: {e}")

def print_report(console: Console, findings: list):
    """Renders the final report to the console using Rich."""
    if not findings:
        console.print(Panel(
            Text("No secrets were found based on the current rules and filters.", style="bold green", justify="center"),
            title="[bold green]Scan Complete[/]",
            border_style="green"
        ))
        return

    grouped_findings = defaultdict(list)
    for f in findings:
        key = f.get('original_path') or f.get('file')
        grouped_findings[key].append(f)

    console.print(Rule(f"[bold yellow]Found {len(findings)} potential secret(s) in {len(grouped_findings)} file(s)[/]", style="yellow"))

    for file_path, file_findings in sorted(grouped_findings.items()):
        table = Table(box=None, show_header=False, padding=(0, 1))
        table.add_column("Info", width=12, style="dim")
        table.add_column("Details")

        file_findings.sort(key=lambda x: x.get('line', 0))

        for finding in file_findings:
            line_num = finding.get('line', '?')
            rule_id = finding.get('rule_id', 'UnknownRule')
            match_str = finding.get('match', '')
            
            entropy = finding.get('entropy', 0.0)
            entropy_style = "bold red" if entropy > 4.5 else "yellow" if entropy > 3.5 else "dim"
            entropy_text = f" (Entropy: [{entropy_style}]{entropy:.2f}[/{entropy_style}])"

            # --- START OF THE FIX ---
            # Instead of appending to a Text object, we create a Group of renderables.
            # A Group can contain any mix of Rich renderables.
            details_group = Group(
                Text.from_markup(f"[cyan]L{line_num}: [/][bold magenta][{rule_id}] [/]"),
                Syntax(f'"{match_str}"', "python", theme="monokai", word_wrap=True),
                Text.from_markup(entropy_text)
            )
            # --- END OF THE FIX ---
            
            row_header = ""
            if 'commit' in finding:
                commit_hash = finding['commit'][:8]
                row_header = Text(f"Commit: {commit_hash}", justify="right")

            table.add_row(row_header, details_group) # The table cell can contain a Group
        
        panel_title = f"[bold cyan]File:[/][default] {file_path}"
        if 'commit' in file_findings[0]:
             panel_title = f"[bold yellow]Git History Finding[/]\n[bold cyan]File:[/][default] {file_path}"
        
        console.print(Panel(table, title=panel_title, border_style="blue", expand=False))

def main():
    parser = argparse.ArgumentParser(prog="secret-hound", add_help=False)
    parser.add_argument("path", nargs='?', default=".", help="File or directory to scan.")
    parser.add_argument("-r", "--rules", help="Path to a custom JSON rules file.")
    parser.add_argument("-o", "--output", help="File to save the report (JSON format).")
    parser.add_argument("--scan-git", action="store_true", help="Scan the Git history.")
    parser.add_argument("--depth", type=int, default=100, help="Depth of Git history to scan.")
    parser.add_argument("-h", "--help", action="store_true", help="Show help message.")
    
    args = parser.parse_args()

    if args.help:
        help_file_path = env.ROOT_DIR / "share" / "readme" / "secret-hound.json"
        try:
            with open(help_file_path, 'r') as f:
                render_help(json.load(f))
        except Exception as e:
            env.log.error(f"Could not display help: {e}")
        return 0

    console = Console()
    findings = []
    
    try:
        with Status("[cyan]Scanning...", console=console, spinner="dots") as status:
            backend_cmd = []
            if args.scan_git:
                status.update("[cyan]Scanning Git history...")
                go_analyzer_path = get_tool_path("git_analyzer")
                core_scanner_path = get_tool_path("hound-core")
                backend_cmd = [go_analyzer_path, core_scanner_path, str(args.depth)]
            else:
                status.update("[cyan]Scanning filesystem...")
                core_scanner_path = get_tool_path("hound-core")
                backend_cmd = [core_scanner_path, args.path]
                if args.rules:
                    backend_cmd.extend(["--rules", args.rules])

            for finding in run_backend_process(backend_cmd, console):
                findings.append(finding)
                status.update(f"[cyan]Scanning... Found {len(findings)} secrets so far.")

    except FileNotFoundError as e:
        env.log.error(str(e))
        return 1
    except KeyboardInterrupt:
        env.log.warning("Scan cancelled by user.")
        return 1
    except Exception as e:
        env.log.error(f"An unexpected error occurred: {e}", exc_info=True)
        return 1

    print_report(console, findings)

    if args.output:
        try:
            with open(args.output, 'w', encoding='utf-8') as f:
                json.dump(findings, f, indent=2)
            env.log.success(f"Report successfully saved to {args.output}")
        except Exception as e:
            env.log.error(f"Failed to save report to {args.output}: {e}")
            return 1
    
    return 0

if __name__ == "__main__":
    sys.exit(main())

--- END OF FILE ---

--- START OF FILE rules/default.json ---
--- File: rules/default.json | Language: JSON ---

[
  {
    "id": "AWS_ACCESS_KEY",
    "description": "AWS Access Key ID",
    "regex": "AKIA[0-9A-Z]{16}",
    "confidence": "High"
  },
  {
    "id": "AWS_SECRET_KEY",
    "description": "AWS Secret Access Key",
    "regex": "[Aa][Ww][Ss](.{0,20})?['\\\"][0-9a-zA-Z\\/\\+]{40}['\\\"]",
    "confidence": "Medium"
  },
  {
    "id": "PRIVATE_KEY_PEM",
    "description": "Private Key (PEM format header)",
    "regex": "-----BEGIN (RSA|EC|OPENSSH|PGP) PRIVATE KEY-----",
    "confidence": "High"
  },
  {
    "id": "SLACK_TOKEN",
    "description": "Slack Token (Legacy and OAuth)",
    "regex": "(xox[pboar]-[0-9]{10,13}-[0-9]{10,13}-[0-9]{10,13}-[a-f0-9]{32})",
    "confidence": "Medium"
  },
  {
    "id": "GITHUB_TOKEN",
    "description": "GitHub Personal Access Token",
    "regex": "ghp_[0-9a-zA-Z]{36}",
    "confidence": "High"
  },
  {
    "id": "STRIPE_API_KEY",
    "description": "Stripe API Key",
    "regex": "(sk|pk)_(test|live)_[0-9a-zA-Z]{24,99}",
    "confidence": "High"
  },
  {
    "id": "GENERIC_HIGH_ENTROPY",
    "description": "Generic secret (high entropy string)",
    "regex": "[a-zA-Z0-9\\-_\\.!@#$%^&*()+=]{20,64}",
    "confidence": "Low",
    "min_entropy": 4.5
  },
  {
    "id": "BASIC_AUTH_URL",
    "description": "URL with embedded credentials",
    "regex": "[a-zA-Z]+://[^\\s:@/]+:[^\\s:@/]+@[^\\s]+",
    "confidence": "Medium"
  }
]

--- END OF FILE ---

--- START OF FILE src/git_analyzer/main.go ---
--- File: src/git_analyzer/main.go | Language: Go ---

/**
 * @file main.go
 * @brief A Go program to analyze Git history for secrets.
 *
 * This tool iterates through a Git repository's history up to a specified depth.
 * For each modified or added file in each commit, it extracts the file's content
 * (blob) and invokes the C++ core scanner (`secret-hound --scan-file`) on it.
 *
 * It then enriches the raw JSON output from the C++ scanner with Git-specific
 * metadata (commit hash, original file path) and prints the final combined
 * JSON object to stdout, ready to be consumed by the Python reporter.
 */

package main

import (
	"bufio"
	"fmt"
	"io/ioutil"
	"os"
	"os/exec"
	"strconv"
	"strings"
	"sync"
)

/**
 * @struct fileBlob
 * @brief Represents a single version of a file (a blob) from a specific commit.
 */
type fileBlob struct {
	hash    string // The Git blob hash of the file content
	path    string // The original path of the file in the repository
	commit  string // The hash of the commit this version belongs to
}

/**
 * @brief Main entry point for the Git analyzer.
 */
func main() {
	if len(os.Args) < 3 {
		fmt.Fprintln(os.Stderr, "Usage: git_analyzer <path_to_hound_core> <depth>")
		os.Exit(1)
	}
	houndCorePath := os.Args[1]
	depthStr := os.Args[2]
	depth, err := strconv.Atoi(depthStr)
	if err != nil {
		depth = 100 // Default to a safe depth if parsing fails
	}

	// 1. Get a list of all file blobs from the git history.
	blobs, err := getGitBlobs(depth)
	if err != nil {
		fmt.Fprintf(os.Stderr, "Error getting git blobs: %v\n", err)
		os.Exit(1)
	}
	
	// Use a map to track scanned content hashes, preventing redundant scans of identical files.
	scannedHashes := make(map[string]bool)
	
	// 2. Set up a concurrent pipeline using a work queue (buffered channel) and worker goroutines.
	var wg sync.WaitGroup
	blobChan := make(chan fileBlob, len(blobs))

	numWorkers := 4 // A reasonable number of concurrent file scanners
	wg.Add(numWorkers)

	for i := 0; i < numWorkers; i++ {
		go func() {
			defer wg.Done()
			for blob := range blobChan {
				if _, exists := scannedHashes[blob.hash]; exists {
					continue // Skip if this exact content has already been scanned
				}
				scannedHashes[blob.hash] = true
				
				scanBlobContent(houndCorePath, blob)
			}
		}()
	}

	// 3. Feed the work queue with all the collected blobs.
	for _, blob := range blobs {
		blobChan <- blob
	}
	close(blobChan) // Signal to workers that no more jobs will be added.

	wg.Wait() // Wait for all worker goroutines to complete.
}

/**
 * @brief Retrieves a list of all unique file blobs within the specified commit depth.
 * It parses the output of `git log` to find added/modified files and then uses
 * `git ls-tree` to get their corresponding blob hashes.
 * @param depth The maximum number of commits to look back.
 * @return A slice of fileBlob structs and an error if one occurred.
 */
func getGitBlobs(depth int) ([]fileBlob, error) {
	cmd := exec.Command("git", "log", fmt.Sprintf("--max-count=%d", depth), "--name-status", "--pretty=format:COMMIT %H", "--no-renames")
	
	stdout, err := cmd.StdoutPipe()
	if err != nil {
		return nil, err
	}
	if err := cmd.Start(); err != nil {
		return nil, err
	}

	var blobs []fileBlob
	var currentCommit string
	scanner := bufio.NewScanner(stdout)

	for scanner.Scan() {
		line := scanner.Text()
		parts := strings.Fields(line)
		
		if len(parts) > 1 && parts[0] == "COMMIT" {
			currentCommit = parts[1]
			continue
		}
		
		// We only care about Added ('A') or Modified ('M') files.
		if len(parts) > 1 && (parts[0] == "A" || parts[0] == "M") {
			filePath := parts[1]
			// Get the blob hash for the file within its specific commit.
			blobHashCmd := exec.Command("git", "ls-tree", currentCommit, filePath)
			output, err := blobHashCmd.Output()
			if err == nil {
				treeParts := strings.Fields(string(output))
				if len(treeParts) > 2 {
					blobs = append(blobs, fileBlob{
						hash:   treeParts[2],
						path:   filePath,
						commit: currentCommit,
					})
				}
			}
		}
	}
	
	if err := cmd.Wait(); err != nil {
		// Suppress exit code 1, which can happen in empty repos.
        if exitErr, ok := err.(*exec.ExitError); ok && exitErr.ExitCode() == 1 {
            // This is not a fatal error.
        } else {
		    return nil, err
        }
	}
	
	return blobs, nil
}

/**
 * @brief Scans the content of a single Git blob for secrets.
 * It writes the blob's content to a temporary file and then executes the
 * C++ core scanner on that file.
 * @param houndCorePath The path to the C++ core scanner executable.
 * @param blob The fileBlob to scan.
 */
func scanBlobContent(houndCorePath string, blob fileBlob) {
	// Create a temporary file to hold the blob's content.
	tmpfile, err := ioutil.TempFile("", "secret-hound-git-*.tmp")
	if err != nil {
		return
	}
	defer os.Remove(tmpfile.Name())

	// Get the content of the blob from git using 'cat-file'.
	contentCmd := exec.Command("git", "cat-file", "-p", blob.hash)
	content, err := contentCmd.Output()
	if err != nil {
		return
	}
	tmpfile.Write(content)
	tmpfile.Close()

	// Execute the C++ core scanner in its internal, single-file mode.
	scanCmd := exec.Command(houndCorePath, "--scan-file", tmpfile.Name())
	
	output, err := scanCmd.Output()
	if err != nil {
		fmt.Fprintf(os.Stderr, "Go analyzer: core scanner failed on blob %s: %v\n", blob.hash, err)
		return
	}
	
	// Process each line of JSON output from the core scanner.
	scanner := bufio.NewScanner(strings.NewReader(string(output)))
	for scanner.Scan() {
		// Enrich the raw JSON finding with Git context and print it.
		// The result is a new, more detailed JSON object.
		fmt.Printf("{\"commit\": \"%s\", \"original_path\": \"%s\", %s\n",
			blob.commit,
			blob.path,
			scanner.Text()[1:], // Efficiently skip the opening '{' of the inner JSON.
		)
	}
}

--- END OF FILE ---

--- START OF FILE src/hound_core/rule_parser.cpp ---
--- File: src/hound_core/rule_parser.cpp | Language: C++ ---

// File: tools/secret-hound/src/hound_core/rule_parser.cpp
// Description: Implements the logic for parsing secret detection rules from JSON. (UPDATED with correct includes)

#include "rule_parser.hpp"
#include <fstream>
#include <sstream>
#include <stdexcept>

// Include the central SNIPER C utility library
// This block ensures C++ treats the header as a C header
extern "C" {
    #include "sniper_c_utils.h"
    #include "cJSON.h"
}

std::vector<DetectionRule> RuleParser::parse_rules_from_file(const std::string& filepath) {
    // Read the entire file into a string
    std::ifstream file_stream(filepath);
    if (!file_stream.is_open()) {
        throw std::runtime_error("Rule file not found or could not be opened: " + filepath);
    }
    std::stringstream buffer;
    buffer << file_stream.rdbuf();
    std::string file_content = buffer.str();

    // Parse the JSON content using cJSON
    cJSON* json = cJSON_Parse(file_content.c_str());
    if (json == NULL) {
        const char *error_ptr = cJSON_GetErrorPtr();
        std::string error_msg = "Failed to parse rule file. ";
        if (error_ptr != NULL) {
            error_msg += "Error before: " + std::string(error_ptr);
        }
        cJSON_Delete(json);
        throw std::runtime_error(error_msg);
    }

    if (!cJSON_IsArray(json)) {
        cJSON_Delete(json);
        throw std::runtime_error("Rule file must contain a JSON array at the root.");
    }

    std::vector<DetectionRule> rules;
    cJSON* rule_json = NULL;

    // Iterate over the JSON array
    cJSON_ArrayForEach(rule_json, json) {
        DetectionRule rule;

        cJSON* id = cJSON_GetObjectItemCaseSensitive(rule_json, "id");
        cJSON* description = cJSON_GetObjectItemCaseSensitive(rule_json, "description");
        cJSON* regex_str = cJSON_GetObjectItemCaseSensitive(rule_json, "regex");
        cJSON* min_entropy = cJSON_GetObjectItemCaseSensitive(rule_json, "min_entropy");

        if (cJSON_IsString(id) && (id->valuestring != NULL) &&
            cJSON_IsString(regex_str) && (regex_str->valuestring != NULL)) 
        {
            rule.id = id->valuestring;
            rule.regex_str = regex_str->valuestring;
            
            if (cJSON_IsString(description) && (description->valuestring != NULL)) {
                rule.description = description->valuestring;
            } else {
                rule.description = "No description provided.";
            }

            if (cJSON_IsNumber(min_entropy)) {
                rule.min_entropy = min_entropy->valuedouble;
            } else {
                rule.min_entropy = 0.0;
            }

            // Compile the regex for faster execution later
            try {
                rule.compiled_regex.assign(rule.regex_str, std::regex_constants::optimize);
            } catch (const std::regex_error& e) {
                // Skip invalid regex patterns but warn the user
                sniper_log(LOG_WARN, "secret-hound", "Skipping rule '%s' due to invalid regex: %s", rule.id.c_str(), e.what());
                continue;
            }
            rules.push_back(rule);
        } else {
             sniper_log(LOG_WARN, "secret-hound", "Skipping a rule due to missing 'id' or 'regex'.");
        }
    }

    cJSON_Delete(json);
    return rules;
}

--- END OF FILE ---

--- START OF FILE src/hound_core/rule_parser.hpp ---
--- File: src/hound_core/rule_parser.hpp | Language: C++ Header ---

// File: tools/secret-hound/src/hound_core/rule_parser.hpp
// Description: Defines the structures for holding parsed secret-detection rules
// and the interface for parsing them from a JSON file.

#ifndef RULE_PARSER_HPP
#define RULE_PARSER_HPP

#include <string>
#include <vector>
#include <regex>

// Represents a single rule for detecting a secret.
struct DetectionRule {
    std::string id;              // Unique identifier, e.g., "AWS_KEY"
    std::string description;     // Human-readable description
    std::string regex_str;       // The regular expression pattern
    std::regex compiled_regex;   // The compiled regex object for performance
    double min_entropy;          // Minimum Shannon entropy required to match (0 if not used)
};

// The main class responsible for loading and managing rules.
class RuleParser {
public:
    /**
     * @brief Parses a JSON file containing an array of detection rules.
     * @param filepath The path to the JSON rule file.
     * @return A vector of DetectionRule structs.
     * @throws std::runtime_error if the file cannot be read or parsed.
     */
    static std::vector<DetectionRule> parse_rules_from_file(const std::string& filepath);
};

#endif // RULE_PARSER_HPP

--- END OF FILE ---

--- START OF FILE src/hound_core/scanner.cpp ---
--- File: src/hound_core/scanner.cpp | Language: C++ ---

/**
 * @file scanner.cpp
 * @brief Implements the core file scanning logic, including multi-threading,
 *        regex matching, and entropy analysis. (UPDATED for output flushing).
 */

#include "scanner.hpp"
#include <iostream>
#include <fstream>
#include <cmath>
#include <map>
#include <vector>
#include <time.h> // Required for nanosleep

// Include the central SNIPER C utility library
extern "C" {
    #include "sniper_c_utils.h"
}

// Data structure passed to the directory walk callback.
struct WalkData {
    Scanner* scanner_instance;
};

/**
 * @brief Callback function for sniper_directory_walk.
 * This function is called for every file and directory found by the walker.
 */
int directory_walk_callback(const WalkInfo* info, void* user_data) {
    // We are only interested in regular files for scanning.
    if (!S_ISREG(info->stat_info.st_mode)) {
        return 0; // Continue walking
    }

    WalkData* data = (WalkData*)user_data;
    
    // Create the task arguments on the heap because the task runs asynchronously.
    ScanTaskArgs* task_args = new ScanTaskArgs{
        data->scanner_instance,
        std::string(info->full_path)
    };

    // Use the public method to add the file to the thread pool's queue.
    data->scanner_instance->add_scan_task(task_args);
    
    return 0; // Continue walking
}

Scanner::Scanner(std::vector<DetectionRule> rules, int num_threads) 
    : rules(std::move(rules)), active_tasks(0) {
    pool = threadpool_create(num_threads, 4096);
    if (!pool) {
        throw std::runtime_error("Failed to create thread pool.");
    }
}

Scanner::~Scanner() {
    if (pool) {
        threadpool_destroy(pool);
    }
}

void Scanner::scan_directory(const std::string& directory_path) {
    WalkData walk_data = {this};
    WalkOptions options = {.follow_symlinks = false, .skip_hidden = true, .max_depth = -1};
    sniper_directory_walk(directory_path.c_str(), &options, directory_walk_callback, &walk_data);
}

void Scanner::wait_for_completion() {
    while (true) {
        pthread_mutex_lock(&pool->lock);
        // The condition for completion is that the queue is empty AND no tasks are currently running.
        bool done = (pool->count == 0 && active_tasks == 0);
        pthread_mutex_unlock(&pool->lock);
        if (done) {
            break;
        }

        // Use the POSIX-standard nanosleep for a non-busy wait.
        struct timespec sleep_time;
        sleep_time.tv_sec = 0;
        sleep_time.tv_nsec = 100000000; // 100 milliseconds
        nanosleep(&sleep_time, NULL);
    }
}

void Scanner::add_scan_task(ScanTaskArgs* task_args) {
    // Increment the counter BEFORE adding the task.
    active_tasks++;
    if (threadpool_add(pool, &Scanner::scan_file_task_wrapper, task_args) != 0) {
        // If adding fails, decrement the counter to maintain consistency.
        active_tasks--;
        delete task_args;
    }
}

const std::vector<DetectionRule>& Scanner::get_rules() const {
    return rules;
}

double Scanner::calculate_shannon_entropy(const std::string& data) {
    if (data.empty()) {
        return 0.0;
    }
    std::map<char, int> freqs;
    for (char c : data) {
        freqs[c]++;
    }
    double entropy = 0.0;
    double len = static_cast<double>(data.length());
    for (auto const& [key, val] : freqs) {
        double p_x = static_cast<double>(val) / len;
        if (p_x > 0) {
            entropy -= p_x * log2(p_x);
        }
    }
    return entropy;
}

void Scanner::scan_file_task_wrapper(void* args) {
    ScanTaskArgs* task_args = static_cast<ScanTaskArgs*>(args);
    task_args->scanner_instance->scan_file(task_args->file_path);
    delete task_args; 
}

void Scanner::scan_file(const std::string& file_path) {
    std::ifstream file_stream(file_path);
    if (!file_stream.is_open()) {
        active_tasks--;
        return;
    }

    std::string line;
    int line_num = 1;
    while (std::getline(file_stream, line)) {
        for (const auto& rule : rules) {
            std::smatch match;
            std::string::const_iterator search_start(line.cbegin());
            
            while (std::regex_search(search_start, line.cend(), match, rule.compiled_regex)) {
                std::string matched_str = match[0].str();
                
                bool entropy_ok = true;
                double entropy = 0.0;

                if (rule.min_entropy > 0.0) {
                    entropy = calculate_shannon_entropy(matched_str);
                    if (entropy < rule.min_entropy) {
                        entropy_ok = false;
                    }
                }

                if (entropy_ok) {
                    std::lock_guard<std::mutex> lock(output_mutex);
                    
                    // --- START OF CRITICAL FIX ---
                    // Use std::endl to ensure the output buffer is flushed immediately to the pipe.
                    // This guarantees the Python reporter receives the data in real-time and prevents deadlocks.
                    std::cout << "{\"file\": \"" << file_path
                              << "\", \"line\": " << line_num
                              << ", \"rule_id\": \"" << rule.id
                              << "\", \"description\": \"" << rule.description
                              << "\", \"match\": \"" << matched_str
                              << "\", \"entropy\": " << entropy
                              << "}" << std::endl; // <-- std::endl flushes the stream.
                    // --- END OF CRITICAL FIX ---
                }
                search_start = match.suffix().first;
            }
        }
        line_num++;
    }

    // Decrement the counter to signal that this task is complete.
    active_tasks--;
}

--- END OF FILE ---

--- START OF FILE src/hound_core/scanner.hpp ---
--- File: src/hound_core/scanner.hpp | Language: C++ Header ---

// File: tools/secret-hound/src/hound_core/scanner.hpp
// Description: Defines the core Scanner class (UPDATED to fix access control errors).

#ifndef SCANNER_HPP
#define SCANNER_HPP

#include "rule_parser.hpp"
#include "threadpool.hpp"
#include <string>
#include <vector>
#include <atomic>
#include <mutex>

// Forward declaration of the Scanner class
class Scanner;

// Structure passed as an argument to each file scanning task in the thread pool.
struct ScanTaskArgs {
    Scanner* scanner_instance; // Pointer back to the main scanner instance
    std::string file_path;           // The path of the file to scan
};

// Main scanner class.
class Scanner {
public:
    /**
     * @brief Constructs a Scanner instance.
     * @param rules A vector of detection rules to use for scanning.
     * @param num_threads The number of worker threads to use.
     */
    Scanner(std::vector<DetectionRule> rules, int num_threads);
    ~Scanner();

    /**
     * @brief Recursively scans a directory for files and adds them to the scan queue.
     * @param directory_path The path to the directory to start scanning from.
     */
    void scan_directory(const std::string& directory_path);
    
    /**
     * @brief Scans a single file for secrets. This is the core worker logic.
     * @param file_path The path of the file to scan.
     */
    void scan_file(const std::string& file_path);
    
    /**
     * @brief Waits for all pending scan tasks in the thread pool to complete.
     */
    void wait_for_completion();

    /**
     * @brief A public method to add a file scanning task to the thread pool.
     * This encapsulates the threadpool_add logic.
     * @param task_args The arguments for the task.
     */
    void add_scan_task(ScanTaskArgs* task_args);

    /**
     * @brief A static wrapper function to be used as a task by the thread pool.
     * It must be public to be accessible as a function pointer.
     * @param args A pointer to a ScanTaskArgs struct.
     */
    static void scan_file_task_wrapper(void* args);

    // Make the rules vector public for access by static task functions.
    const std::vector<DetectionRule>& get_rules() const;

private:
    std::vector<DetectionRule> rules;
    threadpool_t* pool;
    std::atomic<int> active_tasks;
    std::mutex output_mutex; // Mutex to protect cout for thread-safe JSON output

    /**
     * @brief Calculates the Shannon entropy of a given string.
     * @param data The string to analyze.
     * @return The calculated entropy value.
     */
    static double calculate_shannon_entropy(const std::string& data);
};

#endif // SCANNER_HPP

--- END OF FILE ---

--- START OF FILE src/hound_core/threadpool.cpp ---
--- File: src/hound_core/threadpool.cpp | Language: C++ ---

// File: tools/secret-hound/src/hound_core/threadpool.cpp
// Description: Implementation of the thread pool logic.
// Reused from the fastfind tool.

#include "threadpool.hpp"
#include <stdlib.h>
#include <stdio.h>
#include <unistd.h>

// The worker function that each thread executes.
static void *threadpool_worker(void *arg) {
    threadpool_t *pool = (threadpool_t *)arg;
    task_t task;

    while (1) {
        // Acquire the lock to access the queue
        pthread_mutex_lock(&(pool->lock));

        // Wait on the condition variable if the queue is empty and not shutting down
        while (pool->count == 0 && !pool->shutdown) {
            pthread_cond_wait(&(pool->notify), &(pool->lock));
        }

        // If shutting down and the queue is empty, exit the thread
        if (pool->shutdown && pool->count == 0) {
            break;
        }

        // Get the next task from the queue
        task.function = pool->queue[pool->head].function;
        task.argument = pool->queue[pool->head].argument;
        pool->head = (pool->head + 1) % pool->queue_size;
        pool->count--;

        // Release the lock
        pthread_mutex_unlock(&(pool->lock));

        // Execute the task
        (*(task.function))(task.argument);
    }

    // Thread is exiting
    pthread_mutex_unlock(&(pool->lock));
    pthread_exit(NULL);
    return NULL;
}

threadpool_t *threadpool_create(int thread_count, int queue_size) {
    if (thread_count <= 0 || queue_size <= 0) return NULL;

    threadpool_t *pool = (threadpool_t *)malloc(sizeof(threadpool_t));
    if (pool == NULL) return NULL;

    pool->thread_count = 0;
    pool->queue_size = queue_size;
    pool->head = pool->tail = pool->count = 0;
    pool->shutdown = 0;

    pool->threads = (pthread_t *)malloc(sizeof(pthread_t) * thread_count);
    pool->queue = (task_t *)malloc(sizeof(task_t) * queue_size);

    if (pthread_mutex_init(&(pool->lock), NULL) != 0 ||
        pthread_cond_init(&(pool->notify), NULL) != 0 ||
        pool->threads == NULL || pool->queue == NULL) {
        if (pool->threads) free(pool->threads);
        if (pool->queue) free(pool->queue);
        free(pool);
        return NULL;
    }

    // Create worker threads
    for (int i = 0; i < thread_count; i++) {
        if (pthread_create(&(pool->threads[i]), NULL, threadpool_worker, (void *)pool) != 0) {
            threadpool_destroy(pool);
            return NULL;
        }
        pool->thread_count++;
    }
    return pool;
}

int threadpool_add(threadpool_t *pool, task_function_t function, void *argument) {
    if (pool == NULL || function == NULL) return -1;

    pthread_mutex_lock(&(pool->lock));
    if (pool->count == pool->queue_size || pool->shutdown) {
        pthread_mutex_unlock(&(pool->lock));
        return -1; // Queue is full or shutting down
    }

    pool->queue[pool->tail].function = function;
    pool->queue[pool->tail].argument = argument;
    pool->tail = (pool->tail + 1) % pool->queue_size;
    pool->count++;

    // Signal one of the waiting worker threads
    pthread_cond_signal(&(pool->notify));
    pthread_mutex_unlock(&(pool->lock));
    return 0;
}

int threadpool_destroy(threadpool_t *pool) {
    if (pool == NULL) return -1;

    pthread_mutex_lock(&(pool->lock));
    if (pool->shutdown) {
        pthread_mutex_unlock(&(pool->lock));
        return -1; // Already shutting down
    }
    pool->shutdown = 1;

    // Wake up all worker threads
    pthread_cond_broadcast(&(pool->notify));
    pthread_mutex_unlock(&(pool->lock));

    // Wait for all threads to terminate
    for (int i = 0; i < pool->thread_count; i++) {
        pthread_join(pool->threads[i], NULL);
    }

    // Free all allocated resources
    free(pool->threads);
    free(pool->queue);
    pthread_mutex_destroy(&(pool->lock));
    pthread_cond_destroy(&(pool->notify));
    free(pool);
    return 0;
}

--- END OF FILE ---

--- START OF FILE src/hound_core/threadpool.hpp ---
--- File: src/hound_core/threadpool.hpp | Language: C++ Header ---

// File: tools/secret-hound/src/hound_core/threadpool.hpp
// Description: A simple and efficient thread pool implementation for C.
// Reused from the fastfind tool for consistency and reliability.

#ifndef THREADPOOL_H
#define THREADPOOL_H

#include <pthread.h>
#include <stddef.h>

// Represents a single task to be executed by a worker thread.
typedef void (*task_function_t)(void *arg);

typedef struct {
    task_function_t function;
    void *argument;
} task_t;

// The main thread pool structure.
typedef struct {
    pthread_mutex_t lock;       // Mutex for thread-safe access to the queue
    pthread_cond_t notify;      // Condition variable to signal worker threads
    pthread_t *threads;         // Array of worker threads
    task_t *queue;              // The task queue
    int thread_count;           // Number of threads in the pool
    int queue_size;             // Maximum size of the queue
    int head;                   // Queue head pointer
    int tail;                   // Queue tail pointer
    int count;                  // Current number of tasks in the queue
    int shutdown;               // Flag to signal pool shutdown
} threadpool_t;

/**
 * @brief Creates a new thread pool.
 * @param thread_count The number of worker threads to create.
 * @param queue_size The maximum number of tasks that can be queued.
 * @return A pointer to the newly created threadpool_t, or NULL on failure.
 */
threadpool_t *threadpool_create(int thread_count, int queue_size);

/**
 * @brief Adds a new task to the thread pool's queue.
 * @param pool The thread pool to add the task to.
 * @param function The function pointer for the task to be executed.
 * @param argument The argument to be passed to the task function.
 * @return 0 on success, -1 on failure (e.g., pool is full or shutting down).
 */
int threadpool_add(threadpool_t *pool, task_function_t function, void *argument);

/**
 * @brief Shuts down and destroys the thread pool, freeing all resources.
 * @param pool The thread pool to destroy.
 * @return 0 on success, -1 on failure.
 */
int threadpool_destroy(threadpool_t *pool);

#endif // THREADPOOL_H

--- END OF FILE ---

--- START OF FILE src/main.cpp ---
--- File: src/main.cpp | Language: C++ ---

/**
 * @file main.cpp
 * @brief The high-performance C++ core scanner for secret-hound.
 *
 * This executable is designed to be a silent worker. It accepts a path
 * as an argument, scans it for secrets based on rules, and prints any
 * findings as raw, line-delimited JSON to stdout. It is not intended
 * to be called directly by the user, but by a wrapper script.
 */

#include "hound_core/scanner.hpp"
#include "hound_core/rule_parser.hpp"
#include <iostream>
#include <string>
#include <vector>
#include <stdexcept>
#include <unistd.h>
#include <libgen.h>
#include <climits>
#include <sys/stat.h>

extern "C" {
    #include "sniper_c_utils.h"
}

// Prototypes
std::string find_tool_root_path(const char* argv0);

int main(int argc, char* argv[]) {
    if (argc < 2) {
        fprintf(stderr, "Usage: %s <path_to_scan> [--rules /path/to/rules.json]\n", argv[0]);
        return 1;
    }

    const char* target_path = argv[1];
    const char* rules_file_path = NULL;

    // Manual, simple argument parsing for this internal tool.
    for (int i = 2; i < argc; ++i) {
        if (strcmp(argv[i], "--rules") == 0 && i + 1 < argc) {
            rules_file_path = argv[++i];
        }
    }

    try {
        std::string final_rules_path;
        if (rules_file_path) {
            final_rules_path = rules_file_path;
        } else {
            std::string tool_root = find_tool_root_path(argv[0]);
            if (!tool_root.empty()) {
                final_rules_path = tool_root + "/rules/default.json";
            } else {
                sniper_log(LOG_ERROR, "hound-core", "Cannot find default rules file without tool root path.");
                return 1;
            }
        }
        
        auto rules = RuleParser::parse_rules_from_file(final_rules_path);
        int num_threads = sysconf(_SC_NPROCESSORS_ONLN);
        
        Scanner scanner(rules, num_threads);
        
        struct stat s;
        if (stat(target_path, &s) == 0) {
            if (S_ISDIR(s.st_mode)) {
                scanner.scan_directory(target_path);
            } else if (S_ISREG(s.st_mode)) {
                ScanTaskArgs* task_args = new ScanTaskArgs{&scanner, std::string(target_path)};
                scanner.add_scan_task(task_args);
            }
            scanner.wait_for_completion();
        } else {
            sniper_log(LOG_ERROR, "hound-core", "Target path not found: %s", target_path);
            return 1;
        }
    } catch (const std::exception& e) {
        sniper_log(LOG_ERROR, "hound-core", "An exception occurred: %s", e.what());
        return 1;
    }

    return 0;
}

std::string find_tool_root_path(const char* argv0) {
    char real_path_buf[PATH_MAX];
    ssize_t len = readlink("/proc/self/exe", real_path_buf, sizeof(real_path_buf) - 1);
    if (len != -1) {
        real_path_buf[len] = '\0';
    } else if (realpath(argv0, real_path_buf) == nullptr) {
        return "";
    }
    char* path_copy1 = strdup(real_path_buf);
    if (!path_copy1) return "";
    char* exec_dir = dirname(path_copy1);
    
    char* path_copy2 = strdup(exec_dir);
    if (!path_copy2) { free(path_copy1); return ""; }
    char* tool_root_cstr = dirname(path_copy2);
    
    std::string tool_root_path(tool_root_cstr);
    
    free(path_copy1);
    free(path_copy2);
    
    return tool_root_path;
}
--- END OF FILE ---

--- START OF FILE test/secret_hound_test.sh ---
--- File: test/secret_hound_test.sh | Language: Shell Script ---

#!/bin/bash

# ==============================================================================
# SNIPER: secret-hound - Automated Test Suite
# ==============================================================================

# --- Configuration & Globals ---
EXECUTABLE="../tools/secret-hound/bin/secret-hound"
TEST_DIR="hound_test_dir"

# --- Colors for Output ---
C_RED='\033[0;31m'
C_GREEN='\033[0;32m'
C_YELLOW='\033[0;33m'
C_RESET='\033[0m'
C_BOLD='\033[1m'

# --- State Management ---
PASS_COUNT=0
FAIL_COUNT=0

# --- Cleanup Trap ---
trap "echo -e '\n${C_YELLOW}Cleaning up test environment...${C_RESET}'; rm -rf '$TEST_DIR'" EXIT

# --- Helper Functions ---
function run_test() {
    local description="$1"
    local command_str="$2"
    local expected_to_contain="$3"

    echo -e "ðŸ§ª Running Test: ${C_BOLD}${description}${C_RESET}"
    
    # We need to pipe the output to the Python reporter for a full test
    local reporter_path="../tools/secret-hound/ui/reporter.py"
    local full_command="$command_str | python3 $reporter_path"

    local output
    output=$(eval "$full_command" 2>&1)
    
    if echo "$output" | grep -q -- "$expected_to_contain"; then
        echo -e "  ${C_GREEN}[âœ”] PASS${C_RESET}"
        PASS_COUNT=$((PASS_COUNT + 1))
    else
        echo -e "  ${C_RED}[âœ˜] FAIL${C_RESET}: Expected output to contain '${expected_to_contain}'"
        echo -e "--- OUTPUT ---"
        echo "$output"
        echo "--------------"
        FAIL_COUNT=$((FAIL_COUNT + 1))
    fi
    echo ""
}

# --- Test Environment Setup ---
function setup_test_environment() {
    echo -e "${C_YELLOW}Setting up test environment in './${TEST_DIR}'...${C_RESET}"
    rm -rf "$TEST_DIR"
    mkdir -p "$TEST_DIR/config"
    mkdir -p "$TEST_DIR/src"
    
    # File with a clear secret
    echo "const AWS_KEY = 'AKIAIOSFODNN7EXAMPLE';" > "$TEST_DIR/config/keys.js"
    # File with a high-entropy string
    echo "SESSION_SECRET=a1b2c3d4e5f6a7b8c9d0e1f2a3b4c5d6e7f8a9b0c1d2e3f4" > "$TEST_DIR/config/session.env"
    # A normal file
    echo "console.log('Hello World');" > "$TEST_DIR/src/index.js"

    echo -e "${C_YELLOW}Setup complete.${C_RESET}\n"
}

# --- Main Execution ---
function main() {
  if [ ! -x "$EXECUTABLE" ]; then
    echo -e "${C_RED}FATAL: Executable '$EXECUTABLE' not found. Please build the project first.${C_RESET}"
    exit 1
  fi
  
  setup_test_environment

  run_test "Find AWS Key" "$EXECUTABLE $TEST_DIR" "AWS_ACCESS_KEY"
  run_test "Find high entropy generic secret" "$EXECUTABLE $TEST_DIR" "GENERIC_HIGH_ENTROPY"
  run_test "Check total findings count" "$EXECUTABLE $TEST_DIR" "Found 2 potential secret(s)"

  echo "----------------------------------------"
  echo "Tests Finished: ${C_GREEN}${PASS_COUNT} Passed${C_RESET}, ${C_RED}${FAIL_COUNT} Failed${C_RESET}"
  echo "----------------------------------------"

  if [ $FAIL_COUNT -ne 0 ]; then
    exit 1
  fi
}

main

--- END OF FILE ---

--- START OF FILE ui/reporter.py ---
--- File: ui/reporter.py | Language: Python ---

#!/usr/bin/env python3
# ui/reporter.py
# Full, self-contained reporter for Secret Hound output (JSON).
# - Reads JSON list from stdin or from --input <path>
# - Groups matches by file and prints a rich Table inside Panels per file
# - Handles large matches safely, no unsupported args used
# - Optional: --html <path> to save an HTML copy (requires 'rich[html]' to be installed)

from __future__ import annotations
import sys
import json
import argparse
import os
from typing import List, Dict, Any, Optional

from rich.console import Console, Group
from rich.table import Table
from rich.panel import Panel
from rich.syntax import Syntax
from rich.text import Text
from rich.markdown import Markdown

APP_TITLE = "ðŸ” Secret Hound Report"


def load_json_from_stdin() -> Optional[Any]:
    try:
        # Only attempt to read from stdin if there's data piped in
        if not sys.stdin.isatty():
            return json.load(sys.stdin)
    except Exception:
        # ignore parse errors here; caller will handle None
        return None
    return None


def load_json_from_file(path: str) -> Optional[Any]:
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return None


def normalize_data(raw: Any) -> List[Dict[str, Any]]:
    """
    Normalize incoming JSON to a list of entries.
    Expected entry keys: file, line, rule_id, description, match
    Accepts:
    - list of dicts
    - dict containing {"matches": [...]} or {"results": [...]} or {"data": [...]}
    """
    if raw is None:
        return []
    if isinstance(raw, list):
        return raw
    if isinstance(raw, dict):
        for candidate in ("matches", "results", "data", "entries"):
            if candidate in raw and isinstance(raw[candidate], list):
                return raw[candidate]
        # If dict looks like a single entry, wrap it
        # Heuristic: has 'file' and 'match' keys
        if "file" in raw and "match" in raw:
            return [raw]
    return []


def group_by_file(entries: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:
    grouped: Dict[str, List[Dict[str, Any]]] = {}
    for e in entries:
        file_name = e.get("file") or "Unknown file"
        grouped.setdefault(file_name, []).append(e)
    return grouped


def build_table_for_entries(entries: List[Dict[str, Any]]) -> Table:
    table = Table(show_header=True, header_style="bold blue", show_lines=True, expand=True)
    table.add_column("Line", style="cyan", width=8, justify="center")
    table.add_column("Rule ID", style="yellow", width=18, overflow="ellipsis")
    table.add_column("Description", style="green", overflow="fold")
    table.add_column("Match (preview)", style="magenta", overflow="fold")

    for entry in entries:
        line = str(entry.get("line", "-"))
        rule_id = str(entry.get("rule_id", "N/A"))
        description = str(entry.get("description", "")) or "N/A"
        match_str = str(entry.get("match", ""))

        # Use Syntax for a preview; it's safe to place Syntax into a Table cell
        # Do not use unsupported args like `inline`
        # If the match is very long, we shorten the preview for the table cell,
        # and attach a full Syntax renderable below in a collapsible-like separate panel.
        preview = match_str
        max_preview_len = 240
        if len(preview) > max_preview_len:
            short = preview[: max_preview_len - 3] + "..."
            syntax_block = Syntax(f'"{short}"', "python", theme="monokai", word_wrap=True)
        else:
            syntax_block = Syntax(f'"{preview}"', "python", theme="monokai", word_wrap=True)

        table.add_row(line, rule_id, description, syntax_block)

    return table


def print_report(console: Console, grouped: Dict[str, List[Dict[str, Any]]], total: int) -> None:
    console.rule(APP_TITLE)
    panels: List[Panel] = []

    # Sort files for deterministic output
    for file_name in sorted(grouped.keys()):
        entries = grouped[file_name]
        tbl = build_table_for_entries(entries)
        panels.append(
            Panel(
                tbl,
                title=f"[bold cyan]{os.path.basename(file_name)}[/bold cyan]",
                subtitle=f"{len(entries)} potential secret(s)",
                border_style="bright_black",
                padding=(1, 2),
            )
        )

    if not panels:
        console.print("[yellow]No secrets or no input data found.[/yellow]")
        return

    # Print grouped panels
    console.print(Group(*panels))
    console.rule()
    console.print(f"[bold magenta]Total secrets found:[/bold magenta] {total}")


def save_html(console: Console, grouped: Dict[str, List[Dict[str, Any]]], total: int, path: str) -> bool:
    """
    Optionally save a simple HTML version using rich's export_html if available.
    This function will attempt to create a combined markdown/html render.
    """
    try:
        # Build a simple markdown summary and tables as text
        md_lines = [f"# {APP_TITLE}\n", f"**Total secrets found**: {total}\n\n"]
        for file_name in sorted(grouped.keys()):
            md_lines.append(f"## {os.path.basename(file_name)} ({len(grouped[file_name])})\n")
            for entry in grouped[file_name]:
                line = entry.get("line", "-")
                rid = entry.get("rule_id", "N/A")
                desc = entry.get("description", "")
                match_str = entry.get("match", "")
                md_lines.append(f"- **Line {line}** â€” `{rid}` â€” {desc}\n\n")
                # include the match as a code block
                md_lines.append(f"```\n{match_str}\n```\n\n")
        md_text = "".join(md_lines)
        # Convert Markdown to HTML using rich Markdown -> Console.export_html
        from rich.markup import escape
        from rich.panel import Panel as _Panel

        # Render to a temporary console and capture HTML
        # Note: Console().export_html exists in rich; use Console().capture + export_html is not direct.
        # We'll use console.print and Console().export_html if present.
        export_console = Console(record=True)
        export_console.print(Markdown(md_text))
        html = export_console.export_html(title=APP_TITLE)
        with open(path, "w", encoding="utf-8") as f:
            f.write(html)
        return True
    except Exception:
        return False


def parse_args() -> argparse.Namespace:
    p = argparse.ArgumentParser(description="Secret Hound reporter (renders JSON to terminal using rich).")
    p.add_argument("--input", "-i", help="Path to report.json (if not provided, read from stdin)")
    p.add_argument("--html", help="Optional: save an HTML copy to this path")
    return p.parse_args()


def main() -> int:
    args = parse_args()
    console = Console()

    raw = None
    # Priority: stdin (if piped) -> --input path -> interactive prompt
    raw = load_json_from_stdin()
    if raw is None and args.input:
        raw = load_json_from_file(args.input)
    if raw is None and args.input is None:
        # if nothing provided, ask user for path (safe for interactive usage)
        try:
            if sys.stdin.isatty():
                path = input("Enter path to report.json (or leave empty to exit): ").strip()
                if path:
                    raw = load_json_from_file(path)
        except Exception:
            raw = None

    entries = normalize_data(raw)
    grouped = group_by_file(entries)
    total = len(entries)

    print_report(console, grouped, total)

    if args.html:
        ok = save_html(console, grouped, total, args.html)
        if ok:
            console.print(f"[green]Saved HTML report to:[/green] {args.html}")
        else:
            console.print(f"[red]Failed to save HTML report to:[/red] {args.html}")

    return 0


if __name__ == "__main__":
    raise SystemExit(main())
    
--- END OF FILE ---

